
# Мера информации и энтропия

## Часть 1: Энтропия дискретного источника

### Что такое энтропия простыми словами

Представьте, что вы играете в игру "угадай символ". У вас есть коробка с буквами, и вы должны угадать, какую букву вытащат. Если в коробке 50% букв "А" и 50% букв "Б", угадать сложно — высокая неопределённость. Если в коробке 99% букв "А" и 1% букв "Б", угадать легко — низкая неопределённость. **Энтропия измеряет именно эту неопределённость**.

### Математическая формула энтропии

Энтропия рассчитывается по формуле Шеннона:

$$
H(X) = -\sum_{i} p_i \log_2 p_i
$$

Где:

- **H(X)** — это энтропия случайной величины X (измеряется в битах)
- **p_i** — вероятность появления i-го символа
- **∑** — знак суммы, означает "сложить все значения"
- **log₂** — логарифм по основанию 2


### Разбор формулы по шагам

**Шаг 1: Что означает log₂(p_i)**

Логарифм по основанию 2 от вероятности показывает, **сколько бит информации содержит событие**. Например:

- Если p = 1/2, то log₂(1/2) = -1, значит событие содержит 1 бит информации
- Если p = 1/4, то log₂(1/4) = -2, значит событие содержит 2 бита информации
- Если p = 1/8, то log₂(1/8) = -3, значит событие содержит 3 бита информации

**Шаг 2: Почему стоит минус**

Минус нужен потому, что log₂(p) всегда отрицательный (так как вероятность меньше 1). Минус делает итоговую энтропию положительной.

**Шаг 3: Почему умножаем на вероятность**

Мы умножаем количество информации события на его вероятность, чтобы получить **среднее количество информации**. Редкие события дают много информации, но происходят редко. Частые события дают мало информации, но происходят часто.

### Практический пример расчёта

Возьмём простую задачу: у нас есть источник, который генерирует символы:

- Символ "A" появляется с вероятностью 0.75 (75%)
- Символ "B" появляется с вероятностью 0.25 (25%)

```python
import math

# Вероятности символов
p_A = 0.75
p_B = 0.25

# Рассчитаем информацию для каждого символа
info_A = -math.log2(p_A)  # = 0.415 бит
info_B = -math.log2(p_B)  # = 2.000 бит

# Рассчитаем энтропию
H = p_A * info_A + p_B * info_B
print(f"H(X) = {H:.3f} бит")  # = 0.811 бит
```

**Интерпретация результата:**

- Каждый символ "A" содержит 0.415 бит информации
- Каждый символ "B" содержит 2.0 бита информации
- В среднем каждый символ источника содержит 0.811 бит информации


### Свойства энтропии

**1. Неотрицательность:** H(X) ≥ 0. Энтропия не может быть отрицательной.

**2. Максимум при равновероятности:** Для n символов максимальная энтропия равна log₂(n) и достигается, когда все символы равновероятны.

**3. Минимум при определённости:** H(X) = 0, если один из символов имеет вероятность 1, а остальные — 0.

### Применение в программировании

**Сжатие данных:** Энтропия показывает теоретический предел сжатия. Файл с энтропией 3 бита/символ невозможно сжать лучше, чем до 3 бит на символ.

**Генерация паролей:** Пароль с высокой энтропией труднее взломать. Пароль из 8 случайных букв и цифр имеет энтропию ≈ 48 бит.

**Машинное обучение:** В деревьях решений используется энтропия для выбора лучшего разбиения.

***

## Часть 2: Условная энтропия и взаимная информация

### Условная энтропия

**Простое объяснение:** Условная энтропия H(X|Y) показывает, **сколько неопределённости остаётся о переменной X, если мы уже знаем значение переменной Y**.

Представьте погоду и дорожное движение:

- X = дорожное движение (Загруженно/Свободно)
- Y = погода (Дождь/Солнце)

Если мы не знаем погоды, у нас есть некоторая неопределённость о движении. Но если мы узнаём, что идёт дождь, неопределённость уменьшается — в дождь движение чаще загружено.

### Формула условной энтропии

$$
H(X|Y) = -\sum_{x,y} p(x,y) \log_2 p(x|y)
$$

Или через цепное правило:

$$
H(X|Y) = H(X,Y) - H(Y)
$$

Где:

- **H(X,Y)** — совместная энтропия X и Y
- **p(x|y)** — условная вероятность X при известном Y
- **p(x,y)** — совместная вероятность X и Y


### Цепное правило для энтропии

**Основная идея:** Общая неопределённость системы равна сумме неопределённости первой переменной плюс оставшаяся неопределённость второй переменной:

$$
H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
$$

**Пример расчёта:**
Используем данные о погоде и движении:

```python
# Совместные вероятности
# (Движение, Погода): вероятность
joint_prob = {
    ('Загруженно', 'Дождь'): 0.4,
    ('Загруженно', 'Солнце'): 0.1,
    ('Свободно', 'Дождь'): 0.1,
    ('Свободно', 'Солнце'): 0.4
}

# H(X) = 1.000 бит (движение равновероятно)
# H(Y) = 1.000 бит (погода равновероятна)
# H(X,Y) = 1.722 бит (совместная энтропия)
# H(X|Y) = 1.722 - 1.000 = 0.722 бит
```

**Интерпретация:** Знание погоды уменьшает неопределённость о движении с 1.000 до 0.722 бит — экономия 0.278 бит.

### Взаимная информация

**Простое объяснение:** Взаимная информация I(X;Y) измеряет, **насколько знание одной переменной уменьшает неопределённость о другой**.

### Формулы взаимной информации

Существует несколько эквивалентных определений:

$$
I(X;Y) = H(X) - H(X|Y)
$$

$$
I(X;Y) = H(Y) - H(Y|X)
$$

$$
I(X;Y) = H(X) + H(Y) - H(X,Y)
$$

### Свойства взаимной информации

**1. Симметричность:** I(X;Y) = I(Y;X)

**Доказательство простыми словами:** Сколько информации X содержит о Y, столько же Y содержит о X. Это свойство вытекает из математической симметрии формул.

**2. Неотрицательность:** I(X;Y) ≥ 0

Взаимная информация не может быть отрицательной. В худшем случае (при независимости) она равна нулю.

**3. Максимум при функциональной зависимости:**

Если X полностью определяет Y (или наоборот), то I(X;Y) = min(H(X), H(Y)).

**4. Ноль при независимости:**

Если X и Y независимы, то I(X;Y) = 0.

### Практический пример

Из нашего примера с погодой:

- H(X) = 1.000 бит (энтропия движения)
- H(X|Y) = 0.722 бит (энтропия движения при знании погоды)
- I(X;Y) = 1.000 - 0.722 = 0.278 бит

**Интерпретация:** Знание погоды даёт нам в среднем 0.278 бит информации о дорожном движении.

### Применение в программировании

**1. Выбор признаков в ML:**

```python
from sklearn.feature_selection import mutual_info_classif

# Выбираем признаки с наибольшей взаимной информацией
mi_scores = mutual_info_classif(X_features, y_target)
best_features = X_features[:, mi_scores.argsort()[-5:]]  # топ-5
```

**2. Кластеризация:**
Взаимная информация между кластерами должна быть низкой (кластеры независимы), а внутри кластеров — высокой.

**3. Криптоанализ:**
Оценка того, сколько бит ключа раскрывает перехваченный трафик.

**4. Компрессия:**
Файлы с высокой взаимной информацией между соседними байтами лучше сжимаются.

### Цепные правила для взаимной информации

Для нескольких переменных:

$$
I(X_1, X_2, \ldots, X_n; Y) = \sum_{i=1}^{n} I(X_i; Y | X_1, \ldots, X_{i-1})
$$

**Пример для трёх переменных:**

$$
I(X_1, X_2, X_3; Y) = I(X_1; Y) + I(X_2; Y|X_1) + I(X_3; Y|X_1, X_2)
$$

### Заключение

Энтропия и взаимная информация — это фундаментальные инструменты для:

- **Оценки сложности данных** и пределов сжатия
- **Анализа зависимостей** между переменными
- **Выбора информативных признаков** в машинном обучении
- **Проектирования криптографических систем**
- **Понимания информационной структуры** любых данных



